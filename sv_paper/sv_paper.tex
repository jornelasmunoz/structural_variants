% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\input{settings}

% Title.
% ------
\title{AUTHOR GUIDELINES FOR ICASSP 2021 PROCEEDINGS MANUSCRIPTS}
%
% Single address.
% ---------------
\name{Jocelyn Ornelas Mu\~{n}oz$^{\star}$, Erica Rutter$^{\star}$ , Mario Ba\~{n}uelos$^{\dagger}$ , Roummel F. Marcia$^{\star}$}
%\name{Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
\address{$^{\star}$ Department of Applied Mathematics University of California, Merced \\
	$^{\dagger}$Department of Mathematics California State University, Fresno}

\begin{document}
	\ninept
	\maketitle
	
\input{abstract}
	%
	\begin{keywords}
		Structural variants, sparse signal recovery, nonconvex optimization, computational genomics
	\end{keywords}


	\section{INTRODUCTION}
	\label{sec:intro}
	\input{introduction}
		
		
	\section{METHODS}
	\label{sec:methods}
	
%	All printed material, including text, illustrations, and charts, must be kept
%	within a print area of 7 inches (178 mm) wide by 9 inches (229 mm) high. Do
%	not write or print anything outside the print area. The top margin must be 1
%	inch (25 mm), except for the title page, and the left margin must be 0.75 inch
%	(19 mm).  All {\it text} must be in a two-column format. Columns are to be 3.39
%	inches (86 mm) wide, with a 0.24 inch (6 mm) space between them. Text must be
%	fully justified.
	Here, we describe mathematically our computational framework for predicting SVs for related individuals. More specifically, this study only considers diploid data from one parent (P) and one child (C) for mathematical and computational simplicity. We assume that each signal consists of $n$ candidate locations in the genome where an SV may be present. Further, we separate the signal from the child to consider both inherited and novel SVs individually. For this, we denote the true signal of the parent as $ \fP  \in \{0,1,2\}^n $, and the true signal of the child as $\fC = \fH +\fN \in \{0,1,2\}^{n}$, where $\fH \in \{0,1,2\}^n$ and $\fN \in \{0,1,2\}^n$ correspond to the vectors of inherited ($H$) and novel ($N$) structural variants in the child, respectively.
	
%-------------------------------------------------------------------
%                    OBSERVATIONAL MODEL
%-------------------------------------------------------------------
	\subsection{Observational Model}
	\input{observational_model}

%-------------------------------------------------------------------
%                    PROBLEM FORMULATION
%-------------------------------------------------------------------
\subsection{Optimization Formulation}
We will assume a Negative Binomial process to model the noise in the sequencing and mapping measurements. The negative binomial distribution can be parameterized in terms of its mean $\vec{\mu}_l = e_{l}^{T} A \vec{f}$ and standard deviation $\vec{\sigma}_{l}^{\,2} = (e_{l}^{T} A \vec{f})_{l} + \frac{1}{r} (e_{l}^{T} A \vec{f})_{l}^{2} $, $l=1, \dots, 2n$, where $e_l$ represents the canonical standard basis vectors. Note that we drop the arrow in the standard basis vectors for ease of notation. We consider the model with the dispersion parameter set to $r =1$ since the standard deviation is maximized with this choice of $r$. With these considerations, the probability of observing the observation vector $\vec{s}$ given the true signal $\vec{f}$, is given by 
\begin{equation} \label{negBinModel_probability}
	p(\vec{s} \; | A\vec{f})\; = \prod_{l=1}^{2n} \left( \frac{1}{1+ (A \vec{f})_l + \varepsilon} \right) \left(  \frac{((A \vec{f})_l + \varepsilon)}{1+ (A \vec{f})_l + \varepsilon} \right)^{s_l} 
\end{equation}
Here, again,  $\varepsilon>0$ is reflective of the sequencing and mapping errors. \\

The solution space for inferring $\vec{f}$ from $\vec{s}$ is exponentially large for large $n$. Thus, we apply a continuous relaxation  of $\vec{f}$ such that its elements lie between $0$ and $1$, i.e. $\mathbf{0} \leq \vec{f} \leq \mathbf{1}$, or equivalently,
\begin{equation}
	\mathbf{0} \leq \, \vec{z}_i, \vec{y}_i \, \leq \mathbf{1}, \quad i \in \{P,H,N\}.
\end{equation}
For the ease of notation, we assume the inequalities read element-wise and denote the vector of all zeros by $\mathbf{0}$ and the vector of all ones by $\mathbf{1}$. \\

The continuous relaxation of our problem formulation allows us to apply a gradient-based maximum likelihood approach to recover the indicator values $\vec{z}_i$ and $\vec{y}_i$ by estimating $A \vec{f}$ such that the probability of observing the vector of negative binomial data $\vec{s}$ is maximized under our statistical model. 

In particular, we seek to minimize the corresponding Negative Binomial negative log-likelihood function
\begin{equation} \label{negBin_negativeLogLikehihood}
	F(\vec{f}) \equiv \sum_{l=1}^{2n}  (1 + s_l)\log \big(1+ e_l^T A \vec{f} + \varepsilon \big) - s_l \log \big( e_l^T A \vec{f} + \varepsilon \big)
\end{equation}

%-------------------------------------------------------------------
%                    FAMILIAL CONSTRAINTS
%-------------------------------------------------------------------
\subsubsection{Familial Constraints}
We incorporate additional constraints to leverage biological information about $\vec{f}$ to improve accuracy of the model. Since a structural variant cannot be both homozygous and heterozygous at the same time, we require that 
\begin{equation*}
	\mathbf{0} \leq \, \vec{z}_i + \vec{y}_i \, \leq \mathbf{1}, \quad i \in \{P,H,N\}.
\end{equation*}
Recall the signal of the child is comprised of both inherited and novel structural variants, $\fC = \zH + \yH + \zN + \yN$, where a structural variant cannot be both inherited and novel simultaneously. 
\begin{equation*}
	\mathbf{0} \leq \, \zH + \yH + \zN + \yN \, \leq \mathbf{1}.
\end{equation*} 

We consider relatedness in our model. Thus, the child can have an inherited homogeneous SV only if the parent has at least a heterogeneous SV. Similarly, the child can only have an inherited heterogeneous SV if the parent has at least a heterogeneous SV. On the other hand, if the parent has a homogeneous SV at a particular location, then the child must have at least a heterozygous SV at that location, i.e.,
\begin{equation*}
	\mathbf{0} \leq \, \zH   \, \leq \, \zP + \yP  \, \leq \mathbf{1}
\end{equation*}
% \begin{equation*}
	%     \mathbf{0} \leq \, \yH   \, \leq \, \zP + \yP  \, \leq \mathbf{1}
	% \end{equation*}
\begin{equation*}
	\mathbf{0} \leq \, \zP   \, \leq \, \zH + \yH  \, \leq \mathbf{1}
\end{equation*}
Finally, we note that novel structural variants cannot be passed on from the parent. Thus, for a location $j$, if $(\zN)_j + (\yN)_j =1$, then  $(\zP)_j + (\yP)_j =0$. Similarly, if $(\zP)_j + (\yP)_j  =1$, then  $(\zN)_j + (\yN)_j =0$,
\begin{equation*}
	\mathbf{0} \leq \, \zN + \yN   \, \leq \, 1- (\zP +\yP)  \, \leq \mathbf{1}
\end{equation*}
We will denote the set of all vectors satisfying these constraints by $\mathcal{S}$,
% \begin{equation*}
	%      \mathcal{S} =
	%         \begin{Bmatrix*}[l]
		%         &\mathbf{0} \leq \, \vec{z}_i + \vec{y}_i \, \leq \mathbf{1}\\
		%         & \mathbf{0} \leq \, \zH + \yH + \zN + \yN \, \leq \mathbf{1}\\
		%         \vec{f} = [\vec{z}; \vec{y}] \in \R^{6n}: &\mathbf{0} \leq \, \zH   \, \leq \, \zP + \yP  \, \leq \mathbf{1}\\
		%         &\mathbf{0} \leq \, \zH   \, \leq \, \zP + \yP  \, \leq \mathbf{1}\\
		%         &\mathbf{0} \leq \, \zP   \, \leq \, \zH + \yH  \, \leq \mathbf{1}\\
		%         &\mathbf{0} \leq \, \zN + \yN   \, \leq \, 1- (\zP +\yP)  \, \leq \mathbf{1}
		%         \end{Bmatrix*}
	% \end{equation*}
\renewcommand{\arraystretch}{1.3}
\begin{align*}
	\mathcal{S} = \left\{
	\vec{f} = 
	\begin{bmatrix}
		\zP \\ \zH \\ \zN \\ \yP \\ \yH \\ \yN 
	\end{bmatrix} \in \R^{6n} : 
	\begin{matrix*}[l]
		&\mathbf{0} \leq \, \vec{z}_i + \vec{y}_i \, \leq \mathbf{1}\\
		& \mathbf{0} \leq \, \zH + \yH + \zN + \yN \, \leq \mathbf{1}\\
		&\mathbf{0} \leq \, \zH   \, \leq \, \zP + \yP  \, \leq \mathbf{1}\\
		&\mathbf{0} \leq \, \zP   \, \leq \, \zH + \yH  \, \leq \mathbf{1}\\
		&\mathbf{0} \leq \, \zN + \yN   \, \leq \, 1- (\zP +\yP)  \, \leq \mathbf{1}
	\end{matrix*}
	\right\}
\end{align*}. 

%-------------------------------------------------------------------
%                    Sparsity promoting l_1
%-------------------------------------------------------------------
\subsubsection{Sparsity-promoting $\ell_1$ penalty}
Structural variants are rare in an individual's genome. Thus, a common challenge with SV recovery is predicting false positive SVs by mistaking fragments that are incorrectly mapped against the reference genome \cite{MB_diploidTrios}. To model this biological reality, we incorporate an $\ell_1$-norm penalty in our objective function to enforce sparsity in our predictions. Further, we assume novel SVs are even more rare since they are not inherited from a parent. Specifically, we use two penalty terms: one for the parent SVs, $\fP$, and the child's inherited SVs, $\fH$, and another penalty term for the child's novel SVs, $\fN$. We define the penalty as follows:

\begin{equation*}
	\text{pen}(\vec{f}) = (\|\zP\|_1 + \|\zH\|_1 + \|\yP\|_1 + \|\yH\|_1) + \gamma (\|\zN\|_1 + \|\yN\|_1)
\end{equation*}
where $\gamma \gg 1$ is the penalty term that enforces greater sparsity in the child's novel SVs. \\

\noindent Our objective function then takes the following form:

\begin{equation} \label{minimizationProblem}
	\begin{aligned}
		& \underset{\vec{f} \in \R^{6n}}{\text{minimize}}
		& & F(\vec{f}) + \tau \text{pen}(\vec{f}) \\
		& \text{subject to}
		& & \vec{f} \in \mathcal{S}
	\end{aligned}
\end{equation}
where $F(\vec{f})$ is the Negative Binomial negative log-likelihood function shown in (\ref{negBin_negativeLogLikehihood}) and $\tau > 0 $ is a regularization parameter. Our approach in solving the minimization problem (\ref{minimizationProblem}) employs sequential quadratic approximations to the Negative Binomial negative log-likelihood $F(\vec{f})$. More specifically, at iteration $k$, we compute a separable quadratic approximation to $F(\vec{f})$ using its second-order Taylor series approximation at $\vec{f}^k$ and approximate the Hessian matrix by a scalar multiple of the identity matrix, $\alpha_k I$ \cite{Marcia_SPIRALTAP}. This quadratic approximation is then defined as

%% Taylor series approximation
\begin{equation*}
	F^k (\vec{f}) \equiv F(\vec{f}^k) + (\vec{f} - \vec{f}^k)^T \nabla F(\vec{f}^k) + \frac{\alpha_k}{2} \|\vec{f} - \vec{f}^k\|_2^2
\end{equation*}
which we use as a surrogate function for $F(\vec{f})$ in (\ref{minimizationProblem}). Using this approximation, the next iterate is given by 

\begin{equation} \label{iterateOG}
	\vec{f}^{k+1} = 
	\begin{aligned}
		& \underset{\vec{f} \in \R^{6n}}{\text{arg min}}
		& & F^k(\vec{f}) + \tau \text{pen}(\vec{f}) \\
		& \text{subject to}
		& & \vec{f} \in \mathcal{S}
	\end{aligned}
\end{equation}
We can reformulate the constrained quadratic subproblem (\ref{iterateOG}) into the following equivalent sequence of subproblems (see \cite{Marcia_SPIRALTAP}):
\begin{equation} \label{subproblemIterate}
	\vec{f}^{k+1} = 
	\begin{aligned}
		& \underset{\vec{f} \in \R^{6n}}{\text{arg min}}
		& & \mathcal{Q}(\vec{f})= \frac{1}{2}\|\vec{f} - \vec{r}^{\,k}\|_2^2 + \frac{\tau}{\alpha_k} \text{pen}(\vec{f}) \\
		& \text{subject to}
		& & \vec{f} \in \mathcal{S}
	\end{aligned}
\end{equation}
where 
\renewcommand{\arraystretch}{1.4}
\begin{equation*}
	\vec{r}^{\,k} = 
	\begin{bmatrix}
		\vec{r}_{z_P}^{\,k} \\ \vec{r}_{z_H}^{\,k} \\ \vec{r}_{z_N}^{\,k}\\
		\vec{r}_{y_P}^{\,k} \\ \vec{r}_{y_H}^{\,k} \\ \vec{r}_{y_N}^{\,k}
	\end{bmatrix} 
	= \vec{f}^k - \frac{1}{\alpha_k} \nabla F(\vec{f}^k)
\end{equation*}
Our objective function $\mathcal{Q}(\vec{f})$ is separable and decouples into the function
$$ \mathcal{Q}(\vec{f}) = \sum_{j=1}^{n} \mathcal{Q}_j(\zP, \zH, \zN, \yP, \yH, \yN)$$
where 
\begin{align*}
	\mathcal{Q}_j(\zP, \zH, \zN, \yP, \yH, \yN) =& \\
	& \frac{1}{2}\Bigg\{
	((\zP - \vec{r}_{\zP}^{\, k})_j)^2 + 
	((\zH - \vec{r}_{\zH}^{\, k})_j)^2 +
	((\zN - \vec{r}_{\zH}^{\, k})_j)^2 \\ & + 
	((\yP - \vec{r}_{\yP}^{\, k})_j)^2 + 
	((\yH - \vec{r}_{\yH}^{\, k})_j)^2 +
	((\yN - \vec{r}_{\yH}^{\, k})_j)^2 \Bigg\} \\ &+
	\frac{\tau}{\alpha_k} \Big\{ |(\zP)_j| + 
	|(\zH)_j| + \gamma |(\zN)_j| + |(\yP)_j| + 
	|(\yH)_j| + \gamma |(\yN)_j|\Big\}
\end{align*}

Since the bounds that define the region $\mathcal{S}$ are component-wise, then  \ref{subproblemIterate} separates into subproblems of the form

\renewcommand{\arraystretch}{1}
\begin{equation} \label{scalarSubprob}
	\begin{aligned}
		\vec{f}^{k+1} = & \underset{ z_P,z_H,z_N,y_P,y_H,y_N \in \R}{\text{arg min}}
		&  \frac{1}{2} \Big\{
		((\zP - \vec{r}_{\zP}^{\, k})_j)^2 + 
		((\zH - \vec{r}_{\zH}^{\, k})_j)^2 +
		((\zN - \vec{r}_{\zH}^{\, k})_j)^2 \\ & & + 
		((\yP - \vec{r}_{\yP}^{\, k})_j)^2 + 
		((\yH - \vec{r}_{\yH}^{\, k})_j)^2 +
		((\yN - \vec{r}_{\yH}^{\, k})_j)^2 \Big\} \\ & & +
		\frac{\tau}{\alpha_k} \; \Big\{ 
		|(\zP)_j| + |(\zH)_j| + \gamma |(\zN)_j|  +
		|(\yP)_j| + |(\yH)_j| + \gamma |(\yN)_j|\Big\} \\
		& \text{subject to} &  \vec{f} \in S
	\end{aligned}
\end{equation}

where for $i \in \{P,H,N\},\,$ $f_i$ and $r_i$  are scalar components of $\vec{f_i}$ and $\vec{r_i}$, respectively, at the same location.  

Note that \ref{scalarSubprob} has closed form solutions (obtained by completing the square and ignoring constant terms), and thus the constrained minimizer can be easily obtained by projecting the unconstrained solution to the feasible set. 

% Then, the subproblem in (\ref{subproblemIterate}) can be separated into scalar minimization problems. The objective function is separable in $\vec{f}$. Thus, (\ref{subproblemIterate}) decouples into $n$ six-dimensional problems of the form 
% \renewcommand{\arraystretch}{1}
% \begin{equation} %\label{subproblemIterate}
	%     \begin{aligned}
		%         \vec{f}^{k+1} = & \underset{\vec{f} = [z_P;z_H;z_N;y_P;y_H;y_N] \in \R^{6}}{\text{arg min}}
		%         & & \frac{1}{2}\|\vec{f} - r^{k}\|_2^2 + \frac{\tau}{\alpha_k} \text{pen}(\vec{f}) \\
		%         & \text{subject to}
		%         & & \vec{f} \in S
		%     \end{aligned}
	% \end{equation}
% where $r^k = [r^k_{z_P},r^k_{z_H},r^k_{z_N},r^k_{y_P},r^k_{y_H},r^k_{y_N}]^T$ and $f = [z_P,z_H,z_N,y_P,y_H,y_N]^T$ correspond to the components of $\vec{r}^{\,k}$ and $\vec{f}$, respectively, and the set $S$ is similar to the set $\mathcal{S}$ with the distinction that it is restricted to the particular candidate SV position.  

%-------------------------------------------------------------------
%                    OPTIMIZATION APPROACH
%-------------------------------------------------------------------
\subsection{Optimization Approach}
\input{optimization_approach}

%-------------------------------------------------------------------
%                    RESULTS
%-------------------------------------------------------------------
\section{Results}
In order to evaluate the effectiveness of our proposed method, we implemented it in MATLAB by modifying the existing SPIRAL approach \cite{Marcia_SPIRALTAP} to include the negative binomial statistical method \cite{Marcia_SPIRALTAP} to solve the quadratic subproblems. We refer to the new algorithm as NEgative Binomial optimization Using $\ell_1$ penalty Algorithm (NEBULA). We compare SPIRAL and NEBULA. We compared the Poisson-based predictions with the Negative Binomial-based predictions in Sec.\ref{subsec:simulated_data}. The regularization parameters ($\tau, \gamma$) were chosen to obtain the maximum area under the curve (AUC) for the receiver operating characteristic (ROC). The algorithm terminates if the relative difference between consecutive iterates converged to $\|\vec{f}^{k+1} - \vec{f}^{k}\|/\|\vec{f}^{k}\| \leq 10^{-8}$.

\subsection{Simulated Data}
\label{subsec:simulated_data}
\input{results_simulated}

%-------------------------------------------------------------------
%                    CONCLUSION
%-------------------------------------------------------------------
\section{Conclusion}
\input{conclusion.tex}

%-------------------------------------------------------------------
%                    REFERENCES
%-------------------------------------------------------------------
	\vfill\pagebreak
	\bibliographystyle{IEEEbib}
	\bibliography{references}
	
\end{document}
