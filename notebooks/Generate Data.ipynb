{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e9ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Show matplotlib plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f5ffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "desktop_path = '/Users/jocelynornelasmunoz/Desktop/Research/structural_variants/lib/'\n",
    "if desktop_path not in sys.path: sys.path.insert(0, desktop_path)\n",
    "else: sys.path.insert(0, '/Users/jocelynornelas/iCloud Drive (Archive)/Desktop/UC Merced/Research/structural_variants/lib')\n",
    "import generate_data as gd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from scipy.io import savemat\n",
    "import sympy\n",
    "import random\n",
    "\n",
    "# MATLAB\n",
    "# import matlab.engine\n",
    "# import matlab\n",
    "# print(matlab.__file__)\n",
    "# eng = matlab.engine.start_matlab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad47293",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate data and save to `.mat` file\n",
    "1. Define the set of parameters\n",
    "2. Generate diploid data\n",
    "3. Save to `.mat` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a208a2f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'r': 1,\n",
    "    'n': 10**2,\n",
    "    'k': 10,\n",
    "    'lambda_c': 4,\n",
    "    'lambda_p': 4,\n",
    "    'pctNovel': 0.02,\n",
    "    'erreps'  : 1e-2,\n",
    "    #'suffix'  : ['p','c'],\n",
    "    'pct_similarity': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dbf984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop_path = '/Users/jocelynornelas/iCloud Drive (Archive)/Desktop/UC Merced/Research/structural_variants/data/simulated/'\n",
    "desktop_path = '/Users/jocelynornelasmunoz/Desktop/structural_variants/data/simulated/'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa7994e0",
   "metadata": {},
   "source": [
    "data_path = (desktop_path+'%ip_%ic_coverage/')%(params['lambda_p'],params['lambda_c'])\n",
    "if not os.path.exists(data_path): os.mkdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d33f89c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using parameters:\n",
      "\t r :  1\n",
      "\t n :  100\n",
      "\t k :  10\n",
      "\t lambda_c :  4\n",
      "\t lambda_p :  4\n",
      "\t pctNovel :  0.02\n",
      "\t erreps :  0.01\n",
      "\t pct_similarity :  0.5\n",
      "CPU times: user 7.64 ms, sys: 3.04 ms, total: 10.7 ms\n",
      "Wall time: 9.64 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = gd.generate_diploid_data(params)\n",
    "savemat((desktop_path + '%i_%ipctNovel.mat')%(params['n'],params['pctNovel']*100) , data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d674e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['f_p2', 'f_c', 'f_p', 'z_p', 'y_p', 'f_h', 'z_h', 'y_h', 'f_n', 'z_n', 'y_n', 'A_zp', 'A_yp', 'mu_p', 'var_p', 's_p', 'A_zc', 'A_yc', 'mu_c', 'var_c', 's_c', 'r', 'n', 'k', 'lambda_c', 'lambda_p', 'pctNovel', 'erreps', 'pct_similarity'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2da3373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(data['f_c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59c5b471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['f_c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f8906b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Debug `generate_diploid_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62232c5e-378e-4cfe-af5d-5232733fc573",
   "metadata": {},
   "outputs": [],
   "source": [
    "    '''\n",
    "    Generate simulated data for a one parent, one child Structural Variant analysis\n",
    "    Args: A dictionary containing the following parameters as keys\n",
    "        n: size of data vectors (signals)\n",
    "        k: total number of structural variants\n",
    "        pctNovel: percent of novel structural variants in [0,1] (biological reality- very small %)\n",
    "        lambda_p, lambda_c: sequence coverage of child and parent, respectively\n",
    "        erreps: error (>0) incurred by sequencing and mapping process\n",
    "        r: dispersion parameter for Negative Binomial distribution\n",
    "    \n",
    "    Output: A dictionary containing the following data elements as keys\n",
    "    The matrices A_{} are sparse diagonal nxn matrix and I_n is nxn identity matrix\n",
    "        A_zc:   (lambda_c - erreps) I_n,  sparse diagonal nxn matrix. I_n is nxn identity matrix\n",
    "        A_yc: (2*lambda_c - erreps) I_n\n",
    "        A_p: (lambda_p - erreps) I_n, sparse diagonal nxn matrix. I_n is nxn identity matrix\n",
    "        mu_p, var_p: Mean and variance sequence coverage for parent; mu_p = A_p * f_p \n",
    "        mu_c, var_c: Mean and variance sequence coverage for child;  mu_c = A_c * f_c \n",
    "        s_p: nx1 random vector drawn from Negative binomial distribution (for parent)\n",
    "        s_c: nx1 random vector drawn from Negative binomial distribution (for child)\n",
    "        TODO: add mu and var\n",
    "        for i in {P (parent), H (inherited), N (novel)}:\n",
    "        z_i: nx1 indicator vector of homogeneous structural variants\n",
    "        y_i: nx1 indicator vector of heterogeneous structural variants\n",
    "        \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f909124-514b-4400-b10c-b8761448c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'r': 1,\n",
    "    'n': 10**2,\n",
    "    'k': 10,\n",
    "    'lambda_c': 4,\n",
    "    'lambda_p': 4,\n",
    "    'pctNovel': 0.02,\n",
    "    'erreps'  : 1e-2,\n",
    "    #'suffix'  : ['p','c'],\n",
    "    'pct_similarity': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eff8c18-4d78-4623-84e5-4cc10cd925d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we randomly permute a sequence (1,2,3,...n)\n",
    "q = np.random.permutation(np.arange(1,params['n']+1))\n",
    "startVal = int(params['k']*params['pctNovel']); #print(startVal)\n",
    "endVal = int(startVal +params['k']) ; #print(endVal)\n",
    "similarity = int(params['pct_similarity']* params['k']) # pct_similarity * number of SVs\n",
    "\n",
    "signals = ['f_p2', 'f_c']\n",
    "for i,letter in enumerate(['p', 'h', 'n']):\n",
    "    signals.append('f_%s'%letter)\n",
    "    signals.append('z_%s'%letter)\n",
    "    signals.append('y_%s'%letter)\n",
    "\n",
    "# Initialize signals\n",
    "d= {}\n",
    "for signal in signals: d[signal] = np.zeros((params['n'],1), dtype=np.int32)\n",
    "\n",
    "# parent signals: \n",
    "#        f_p  - k elements will be 1s and 2s randomly selected\n",
    "#        f_p2 - floor of %similarity*k elements will be the same as f_p and the rest will be random 1s and 2s\n",
    "for i in q[:params['k']]: d['f_p'][i] = np.random.randint(1,3) \n",
    "d['f_p2'][q[0:similarity]] = d['f_p'][q[0:similarity]]\n",
    "d['f_p2'][random.choices(q, k=params['k'] -similarity)]= np.random.randint(1,3) \n",
    "\n",
    "# child signal\n",
    "#     inherited\n",
    "for i in np.arange(d['f_p'].shape[0]):\n",
    "    if   (d['f_p'][i]==2 and d['f_p2'][i]==2): d['f_h'][i]= 2\n",
    "    elif (d['f_p'][i]==1 and d['f_p2'][i]==1): d['f_h'][i]= np.random.randint(0,3)\n",
    "    elif (d['f_p'][i]==2 and d['f_p2'][i]==0) or (d['f_p'][i]==0 and d['f_p2'][i]==2): d['f_h'][i]= 1\n",
    "    elif (d['f_p'][i]==2 and d['f_p2'][i]==1) or (d['f_p'][i]==1 and d['f_p2'][i]==2): d['f_h'][i]= np.random.randint(1,3)\n",
    "    elif (d['f_p'][i]==1 and d['f_p2'][i]==0) or (d['f_p'][i]==0 and d['f_p2'][i]==1): d['f_h'][i]= np.random.randint(0,2)\n",
    "\n",
    "#     novel        \n",
    "d['f_n'][random.choices(q, k=params['k'] -similarity)]= np.random.randint(1,3) \n",
    "d['f_c'] = d['f_h'] +d ['f_n']\n",
    "\n",
    "# convert signals to indicators\n",
    "for j,letter in enumerate(['p','h','n']):\n",
    "    for i in np.arange(d['f_c'].shape[0]):\n",
    "        if   d['f_%s'%letter][i]==2: d['z_%s'%letter][i]=1\n",
    "        elif d['f_%s'%letter][i]==1: d['y_%s'%letter][i]=1\n",
    "\n",
    "for i, letter in enumerate(['p','c']):\n",
    "    d['A_z%s'%letter]   = (2*params[\"lambda_%s\"%letter] - params['erreps'])*sparse.eye(params['n'])\n",
    "    d['A_y%s'%letter]   = (params[\"lambda_%s\"%letter] - params['erreps'])*sparse.eye(params['n'])\n",
    "    d['mu_%s'%letter]  = np.matmul((d['A_z%s'%letter]+d['A_y%s'%letter]).toarray(), d['f_%s'%letter]) + params['erreps']\n",
    "    d['var_%s'%letter] = d['mu_%s'%letter] +(1/params['r'])*(d['mu_%s'%letter]**2)\n",
    "    d['s_%s'%letter]   = np.random.negative_binomial(d['mu_%s'%letter]/(d['var_%s'%letter]-d['mu_%s'%letter]),d['mu_%s'%letter]/d['var_%s'%letter])\n",
    "\n",
    "data = {**d, **params}\n",
    "print()\n",
    "print('Using parameters:')\n",
    "for key, val in params.items():\n",
    "    print('\\t', key, ': ', val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c5841-ae26-4539-a147-cfb17cff627b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66874810-8254-47a7-b3b5-b2fa6d2324b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b68a5a-c4f8-426a-827d-879adf16d57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7cbc4a-df9f-41d8-9bc0-2304f35cef04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c583b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(d['f_c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29863ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     novel        \n",
    "d['f_n'][random.choices(q, k=int(params['k']*params['pctNovel']))]= np.random.randint(1,3) \n",
    "d['f_c'] = d['f_h'] +d ['f_n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac35876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(d['f_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(d['f_p2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e55a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(d['f_h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c140f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(d['f_n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979ee8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ecf1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert signals to indicators\n",
    "for j,letter in enumerate(['p','h','n']):\n",
    "    for i in np.arange(d['f_c'].shape[0]):\n",
    "        if   d['f_%s'%letter][i]==2: d['z_%s'%letter][i]=1\n",
    "        elif d['f_%s'%letter][i]==1: d['y_%s'%letter][i]=1\n",
    "\n",
    "for i, letter in enumerate(['p','c']):\n",
    "    d['A_z%s'%letter]   = (2*params[\"lambda_%s\"%letter] - params['erreps'])*sparse.eye(params['n'])\n",
    "    d['A_y%s'%letter]   = (params[\"lambda_%s\"%letter] - params['erreps'])*sparse.eye(params['n'])\n",
    "    d['mu_%s'%letter]  = np.matmul((d['A_z%s'%letter]+d['A_y%s'%letter]).toarray(), d['f_%s'%letter]) + params['erreps']\n",
    "    d['var_%s'%letter] = d['mu_%s'%letter] +(1/params['r'])*(d['mu_%s'%letter]**2)\n",
    "    d['s_%s'%letter]   = np.random.negative_binomial(d['mu_%s'%letter]/(d['var_%s'%letter]-d['mu_%s'%letter]),d['mu_%s'%letter]/d['var_%s'%letter])\n",
    "\n",
    "data = {**d, **params}\n",
    "print()\n",
    "print('Using parameters:')\n",
    "for key, val in params.items():\n",
    "    print('\\t', key, ': ', val)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7c71c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Defining generate data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd463d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_haploid_data(params):\n",
    "    '''\n",
    "    Generate simulated data for a one parent, one child Structural Variant analysis\n",
    "    Args: A dictionary containing the following parameters as keys\n",
    "        n: size of data vectors (signals)\n",
    "        k: total number of structural variants\n",
    "        pctNovel: percent of novel structural variants in [0,1] (biological reality- very small %)\n",
    "        lambda_p, lambda_c: sequence coverage of child and parent, respectively\n",
    "        erreps: error (>0) incurred by sequencing and mapping process\n",
    "        r: dispersion parameter for Negative Binomial distribution\n",
    "    \n",
    "    Output: A dictionary containing the following data elements as keys\n",
    "        A_c: (lambda_c - erreps) I_n, sparse diagonal nxn matrix. I_n is nxn identity matrix\n",
    "        A_p: (lambda_p - erreps) I_n, sparse diagonal nxn matrix. I_n is nxn identity matrix\n",
    "        mu_p, var_p: Mean and variance sequence coverage for parent; mu_p = A_p * f_p \n",
    "        mu_c, var_c: Mean and variance sequence coverage for child;  mu_c = A_c * f_c \n",
    "        s_p: nx1 random vector drawn from Negative binomial distribution (for parent)\n",
    "        s_c: nx1 random vector drawn from Negative binomial distribution (for child)\n",
    "        TODO: add mu and var\n",
    "        for i in {P (parent), H (inherited), N (novel)}:\n",
    "        z_i: nx1 indicator vector of homogeneous structural variants\n",
    "        y_i: nx1 indicator vector of heterogeneous structural variants\n",
    "        \n",
    "    '''\n",
    "    q = np.random.permutation(params['n'])\n",
    "    #print(q)\n",
    "    startVal = int(params['k']*params['pctNovel']); #print(startVal)\n",
    "    endVal = int(startVal +params['k']) ; #print(endVal)\n",
    "\n",
    "    f_p, f_c, f_h, f_n = np.zeros((params['n'],1), dtype=np.int8),np.zeros((params['n'],1), dtype=np.int8), np.zeros((params['n'],1), dtype=np.int8), np.zeros((params['n'],1), dtype=np.int8)\n",
    "    f_p[q[: params['k']]], f_c[q[startVal:endVal]] = 1,1\n",
    "    f_h[q[startVal:params['k']+1]], f_n[q[params['k']+1:endVal]] = 1,1\n",
    "    \n",
    "    \n",
    "    d = {}\n",
    "    d['f_p'] = f_p; d['f_h'] = f_h; d['f_n'] = f_n; d['f_c'] = f_h + f_n; \n",
    "    \n",
    "    for i, letter in enumerate(['p','c']):\n",
    "        d['A_%s'%letter]   = (params[\"lambda_%s\"%letter] - params['erreps'])*sparse.eye(params['n'])\n",
    "        d['mu_%s'%letter]  = np.matmul(d['A_%s'%letter].toarray(), d['f_%s'%letter]) + params['erreps']\n",
    "        d['var_%s'%letter] = d['mu_%s'%letter] +(1/params['r'])*(d['mu_%s'%letter]**2)\n",
    "        d['s_%s'%letter]   = np.random.negative_binomial(d['mu_%s'%letter]/(d['var_%s'%letter]-d['mu_%s'%letter]),d['mu_%s'%letter]/d['var_%s'%letter])\n",
    "    \n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ca9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'r': 1,\n",
    "    'n': 10**2,\n",
    "    'k': 10,\n",
    "    'lambda_c': 4,\n",
    "    'lambda_p': 8,\n",
    "    'pctNovel': 0.15,\n",
    "    'erreps'  : 1e-2,\n",
    "    #'suffix'  : ['p','c'],\n",
    "    'pct_similarity': 0.6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a7cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diploid_data(params):\n",
    "    q = np.random.permutation(params['n'])\n",
    "    startVal = int(params['k']*params['pctNovel']); #print(startVal)\n",
    "    endVal = int(startVal +params['k']) ; #print(endVal)\n",
    "    similarity = int(params['pct_similarity']* params['k']) # pct_similarity * number of SVs\n",
    "\n",
    "    signals = ['f_p2', 'f_c']\n",
    "    for i,letter in enumerate(['p', 'h', 'n']):\n",
    "        signals.append('f_%s'%letter)\n",
    "        signals.append('z_%s'%letter)\n",
    "        signals.append('y_%s'%letter)\n",
    "\n",
    "    # Initialize signals\n",
    "    d= {}\n",
    "    for signal in signals: d[signal] = np.zeros((params['n'],1), dtype=np.int32)\n",
    "\n",
    "    # parent signals: \n",
    "    #        f_p  - k elements will be 1s and 2s randomly selected\n",
    "    #        f_p2 - floor of %similarity*k elements will be the same as f_p and the rest will be random 1s and 2s\n",
    "    for i in q[:params['k']]: d['f_p'][i] = np.random.randint(1,3) \n",
    "    d['f_p2'][q[0:similarity]] = d['f_p'][q[0:similarity]]\n",
    "    d['f_p2'][random.choices(q, k=params['k'] -similarity)]= np.random.randint(1,3) \n",
    "\n",
    "    # child signal\n",
    "    #     inherited\n",
    "    for i in np.arange(d['f_p'].shape[0]):\n",
    "        if   (d['f_p'][i]==2 and d['f_p2'][i]==2): d['f_c'][i]= 2\n",
    "        elif (d['f_p'][i]==1 and d['f_p2'][i]==1): d['f_c'][i]= np.random.randint(0,3)\n",
    "        elif (d['f_p'][i]==2 and d['f_p2'][i]==0) or (d['f_p'][i]==0 and d['f_p2'][i]==2): d['f_c'][i]= 1\n",
    "        elif (d['f_p'][i]==2 and d['f_p2'][i]==1) or (d['f_p'][i]==1 and d['f_p2'][i]==2): d['f_c'][i]= np.random.randint(1,3)\n",
    "        elif (d['f_p'][i]==1 and d['f_p2'][i]==0) or (d['f_p'][i]==0 and d['f_p2'][i]==1): d['f_c'][i]= np.random.randint(0,2)\n",
    "        \n",
    "    #     novel        \n",
    "    d['f_n'][random.choices(q, k=params['k'] -similarity)]= np.random.randint(1,3) \n",
    "    d['f_c'] = d['f_h'] +d ['f_n']\n",
    "    \n",
    "    # convert signals to indicators\n",
    "    for j,letter in enumerate(['p','h','n']):\n",
    "        for i in np.arange(d['f_c'].shape[0]):\n",
    "            if   d['f_%s'%letter][i]==2: d['z_%s'%letter][i]=1\n",
    "            elif d['f_%s'%letter][i]==1: d['y_%s'%letter][i]=1\n",
    "            \n",
    "    for i, letter in enumerate(['p','c']):\n",
    "        d['A_%s'%letter]   = (params['lambda_%s'%letter] - params['erreps'])*sparse.eye(params['n'])\n",
    "        d['mu_%s'%letter]  = np.matmul(d['A_%s'%letter].toarray(), d['f_%s'%letter]) + params['erreps']\n",
    "        d['var_%s'%letter] = d['mu_%s'%letter] +(1/params['r'])*(d['mu_%s'%letter]**2)\n",
    "        d['s_%s'%letter]   = np.random.negative_binomial(d['mu_%s'%letter]/(d['var_%s'%letter]-d['mu_%s'%letter]),d['mu_%s'%letter]/d['var_%s'%letter])\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37114f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "hap = generate_haploid_data(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f607a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "hap.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff3a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c01fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,letter in enumerate(['p','h','n']):\n",
    "    print('f_%s: \\n'%letter,np.transpose(d['f_%s'%letter]))\n",
    "    print('z_%s: \\n'%letter,np.transpose(d['z_%s'%letter]))\n",
    "    print('y_%s: \\n'%letter,np.transpose(d['y_%s'%letter]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = generate_diploid_data(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619cbcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9113f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.transpose(data1['f_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66cf4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.transpose(data1['s_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed2ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.transpose(data1['f_p'] - data1['s_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f753a15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
